
\section{Introduction}

In 2007, David Blei and Jon MacAuliffe presented an interesting
extension to LDA, supervised LDA (sLDA) \cite{slda}.  In their paper,
a variational EM algorithm is proposed to train the model.  In this
chapter, I derive a Gibbs sampling algorithm to train the same model.
Notations used in the following
derivation:
\begin{table}[!h]
  \centering
  \begin{tabular}{ll}
    $V$ & the size of vocabulary \\
    $K$ & the number of topics \\
    $D$ & the number of documents in the training corpus \\
    $N_d$ & the number of words in document $d$ \\
    $w_{d,n}$ & the $n$-th word in document $d$ \\
    $W$ & $W=\{w_{d,n}\}$, all words in the training corpus \\
    $z_{d,n}$ & the topic of the $n$-th word in document $d$ \\
    $Z$ & $Z=\{z_{d,n}\}$, all topic assignments for the corpus \\
    $\omega_{d,k}$ & the number words assigned topic $k$ in document $d$ \\
    $\Omega$ & $\Omega=\{\omega_{d,k}\}$, a $D\times{}K$ matrix \\
    $\theta_d$ & the topic distribution of document $d$ \\
    $\Theta$ & $\Theta=\{\theta_d\}_{d=1}^D$ \\
    $\alpha$ & the Dirichlet parameter vector that generates $\Theta$ \\
    $y_d$ & the rate of docuemnt $d$ \\
    $Y$ & $Y=\{y_d\}$, rate of all documents \\
    $\psi_{k,v}$ & the number of times that word $v$ is assigned topic $k$ \\
    $\Psi$ & $\Psi=\{\psi_{k,v}\}$, a $K\times{}V$ matrix \\
    $\phi_k$ & $\phi_k=\{\phi_{k,v}=p(w=v|z=k)\}$, the word distribution of topic $k$ \\
    $\Phi$ & $\Phi=\{\phi_k\}_{k=1}^K$ \\
    $\beta$ & the Dirichlet parameter vector that generates $\Phi$ \\
    $R=\{\eta, \mu, \sigma^2\}$ & transformer, basis and covariance of the
    rate model \\
  \end{tabular}
\end{table}

Using these notations, Figure~\ref{fig:slda} and
Figure~\ref{fig:slda-set} illustrate the Bayesian network of sLDA.

\begin{figure}[!t]
  \centering
  \includegraphics[scale=0.8]{slda}
  \caption{The Bayesian network of sLDA.}
  \label{fig:slda}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[scale=0.8]{slda-set}
  \caption{An equivalent Bayesian network of sLDA using set notations.}
  \label{fig:slda-set}
\end{figure}

\section{Gibbs Sampling}

Our training algorithm iteratively executes two steps: (i) Given model
parameters, $\Phi$ and $R$, estimate the values of hidden variables,
$Z$; and (ii) given $Z$, update the model parameters.  Gibbs sampling
is used in (i) to estimate the value of each $z_{d,n}\in{}Z$ by
sampling from the full conditional distribution of $z_{d,n}$:
\begin{equation}
  \label{eq:slda-full-conditional}
  p(z_{d,n}|Z_{-(d,n)},W,Y,\alpha,\beta,R)
  =
  \frac{
    p(z_{d,n},Z_{-(d,n)},W,Y|\alpha,\beta,R)
  }{
    p(Z_{-(d,n)},W,Y|\alpha,\beta,R)
  }
  \enspace.
\end{equation}
Here $Z_{-(d,n)}$ denotes a subset of $Z$ with $z_{d,n}$ excluded, or
in the notation of set theory, $Z\backslash\{z_{d,n}\}$.  Because
$w_{d,n}$ is generated from $z_{d,n}$, when $z_{d,n}$ is excluded from
$Z$, $w_{d,n}$ can be excluded from $W$.  So
(\ref{eq:slda-full-conditional}) is equivalent to
\begin{equation}
  \label{eq:slda-full-conditional-1}
  p(z_{d,n}|Z_{-(d,n)},W,Y,\alpha,\beta,R)
  =
  \frac{
    p(Z,W,Y|\alpha,\beta,R)
  }{
    p(Z_{-(d,n)},W_{-(d,n)},Y|\alpha,\beta,R)
  }
  \enspace.
\end{equation}

Refer to Figure~\ref{fig:slda-set}, we can write the numerator at the
right side of (\ref{eq:slda-full-conditional-1}) as
\begin{equation}
  \label{eq:slda-joint-distribution}
  p(Z,W,Y|\alpha,\beta,R) =
  p(Z|\alpha) \; p(W|Z,\beta) \; p(Y|Z,R)
  \enspace.
\end{equation}

The conjugacy between Dirichlet prior and multinomial likelihood makes
it convenient to compute $p(Z|\alpha)$ and $p(W|Z,\beta)$, similar
with what we did for LDA. (For details please refer \cite{heinrich} or
(\ref{eq:term1})$\sim$(\ref{eq:joint-dist-final}) in this document.)
\begin{equation}
  \label{eq:gen-Z}
  p(Z|\alpha)
  =
  \int p(Z|\Theta,\alpha) \; p(\Theta|\alpha) \; \ud\Theta
  =
  \prod_{d=1}^D
  \frac{\B(\Omega_d+\alpha)}{\B(\alpha)}
\end{equation}
\begin{equation}
  \label{eq:gen-W}
  p(W|Z,\beta)
  =
  \int P(W|\Phi,Z,\beta) \; p(\Phi|Z,\beta) \; \ud\Phi
  =
  \prod_{k=1}^K
  \frac{\B(\Psi_k+\beta)}{\B(\beta)}
\end{equation}
where $\Omega_d$ is the $d$-th row of a $D\times{}K$ matrix, $\Omega$,
where $\Omega_{d,k}$ is the number of words assigned topic $k$ in
document $d$; $\Psi_k$ is the $k$-th row of a $K\times{}V$ matrix,
$\Psi$, where $\Psi_{k,w}$ is the number of times that word $w$ is
assigned topic $k$ in the training corpus; $B(\cdot)$ is the gamma
function (ref. (\ref{eq:beta-function})).

The last factor of (\ref{eq:slda-joint-distribution}) is
\begin{equation}
  \label{eq:gen-rate}
  p(Y|Z,R) =
  \prod_{d=1}^D p(y_d|Z_d,R) =
  \prod_{d=1}^D \N(y_d; \; \eta \cdot \hat{\theta}_d + \mu, \sigma^2)
\end{equation}
where $\hat{\theta}_d$ is the empirical topic distribution of document
$d$, which can be computed by unifying $\Omega_d$.

The denominator at the right side of
(\ref{eq:slda-full-conditional-1}) has similar form of the numerator:
\begin{equation}
  \label{eq:slda-sub-joint-distribution}
  \begin{split}
    p(&Z_{-(d,n)},W_{-(d,n)},Y|\alpha,\beta,R) = \\
    &p(Z_{-(d,n)}|\alpha) \; p(W_{-(d,n)}|Z_{-(d,n)},\beta) \; p(Y|Z_{-(d,n)},R)
  \end{split}
\end{equation}
where
\begin{equation}
  \label{eq:gen-sub}
  \begin{split}
  p(Z_{-(d,n)}|\alpha)
  &=
  \prod_{d=1}^D
  \frac{\B(\Omega_{d,-n}+\alpha)}{\B(\alpha)}
  \\
  p(W_{-(d,n)}|Z_{-(d,n)},\beta)
  &=
  \prod_{k=1}^K
  \frac{\B(\Psi_{k,-n}+\beta)}{\B(\beta)}
  \\
  p(Y|Z_{-(d,n)},R)
  &=
  \prod_{d=1}^D \N(y_d; \; \eta \cdot \hat{\theta}_{d,-n} + \mu, \sigma^2)
  \end{split}
\end{equation}
where $\Omega_{d,-n}$ is $\Omega_{d}$ with the topic assigned to the
$n$-th word excluded; $\Psi_{d,-n}$ is $\Psi_d$ with the topic
assigned to the $n$-th word excluded; $\theta_{d,-n}$ is the
unification of $\Omega_{d,-n}$\footnote{Note that a document must
  contains at least two words, otherwise $\hat{\theta}_{d,-n}$
  degenerates to a zero-vector.  However, this requirement is not
  specific to sLDA, it should also be hold for LDA, because the basic
  idea of LDA is to cluster frequently co-occurred words into a topic;
  empty documents and documents containing one word do not contribute
  to this goal.}.

Given (\ref{eq:slda-joint-distribution})$\sim$(\ref{eq:gen-sub}), the
full conditional distribution can be written as
\begin{equation}
  \label{eq:slda-full-conditional-1}
  \begin{split}
  p&(z_{d,n}|Z_{-(d,n)},W,Y,\alpha,\beta,R)
  \\
  =&
  \frac{
    p(Z,W,Y|\alpha,\beta,R)
  }{
    p(Z_{-(d,n)},W_{-(d,n)},Y|\alpha,\beta,R)
  }
  \\
  =&
  \left[ \frac{p(Z|\alpha)}{p(Z_{-(d,n)}|\alpha)} \right] \cdot
  \left[ \frac{p(W|Z,\beta)}{p(W_{-(d,n)}|Z_{-(d,n)},\beta)} \right] \cdot
  \left[ \frac{p(Y|Z,R)}{p(Y|Z_{-(d,n)},R)} \right]
  \\
  =&
  \left[ \frac{
    \Omega_{d,z_{d,n}} + \alpha_{z_{d,n}} - 1
  }{
    \left( \sum_{k=1}^K  \Omega_{d,k} + \alpha_k \right) - 1
  } \right]
  \cdot
  \left[ \frac{
    \Psi_{z_{d,n},w_{d,n}} + \beta_{w_{d,n}} - 1
  }{
    \left( \sum_{v=1}^V  \Psi_{v,z_{d,n}} + \beta_w \right) - 1
  } \right]
  \cdot
  \\
  &
  \left[ \frac{
    \N(y_d; \; \eta \cdot \hat{\theta}_d + \mu, \sigma^2)
  }{
    \N(y_d; \; \eta \cdot \hat{\theta}_{d,-n} + \mu, \sigma^2)
  } \right]
  \enspace.
  \end{split}
\end{equation}

\section{Parameter Estimation}

The model parameters includes two part: (i) the language model
$\Phi=p(v|k)$ for $1\leq{}v\leq{}V$ and $1\leq{}k\leq{}K$, and (ii)
the rate model $R=\{\eta,\mu,\sigma^2\}$.

In the Gibbs sampling step, we need to update the two count matrices,
$\Omega$ and $\Psi$.  One of them, $\Psi$, is a representation of
$\Phi$.  According to the physical meaning of parameters of the
Dirichlet-multinomial distribution \cite{prml}, we can unifying the
$\Psi_k$, the $k$-th row of $\Psi$, using $\beta$ as the Dirichlet
smoothing factor to estimate the word distribution of topic $k$:
\begin{equation}
  \label{eq:read-out-final}
  \phi_{k,v}
  =
  \frac{
    \Psi_{v,k} + \beta_{v}
  }{
    \left( \sum_{v'=1}^V  \Psi_{v',k} + \beta_{v'} \right)
  }
  \enspace.
\end{equation}

The similar technique can be used to estimate the empirical topic
distributions of documents, $\hat\Theta=\{\hat\theta_d\}$:
\begin{equation}
  \hat\theta_{d,k}
  =
  \frac{
    \Omega_{k,d} + \alpha_k
  }{
    \left( \sum_{k'=1}^K  \Omega_{k',d} + \alpha_k \right)
  }
  \enspace.
\end{equation}

Given $\hat\Theta=\{\hat\theta_d\}$ and $Y=\{y_d\}$, we can estimate
$R$ by maximizing likelihood of the rate model:
\begin{equation}
  \label{eq:rate-model}
  \log p(Y | \hat\Theta, R) =
  \sum_{d=1}^D \log \N(y_d; \; \eta \cdot \hat\theta_d + \mu, \sigma^2)
\end{equation}
Denoting
\begin{equation}
 \Lambda\equiv[\eta\;\mu] \qquad
 \Pi_d\equiv\begin{bmatrix}\hat\theta_d\\1\end{bmatrix} \qquad
 \hat\mu_d \equiv \eta \cdot \hat\theta_d + \mu
\end{equation}
we have
\begin{equation}
  \label{eq:ml-rate-model}
  \begin{split}
  \frac{\partial{\log p(Y | \hat\Theta, R)}}{\partial \Lambda}
  &=
  -\frac{1}{2}
  \sum_{d=1}^D
  \frac{\partial}{\partial \Lambda}
  \left(y_d - \hat\mu_d\right)^T \frac{1}{\sigma^2} \left(y_d - \hat\mu_d\right)
  \\
  &= \frac{1}{\sigma^2} \sum_{d=1}^D \left[ y_d \Pi_d^T - \Lambda\Pi\Pi^T \right]
  \end{split}
\end{equation}
Setting this derivative to zero and solving for $\Lambda$, we get the
update equation for $\Lambda$:
\begin{equation}
  \label{eq:update-slda-transformer-basis}
  \Lambda = \left[\sum_d y_d\Pi_d^T\right] \left[\sum_d \Pi_d\Pi_d^T\right]
\end{equation}
For skills used to derive (\ref{eq:ml-rate-model}) and
(\ref{eq:update-slda-transformer-basis}), please refer to \cite{phmm}
and \cite{psomn}.  Once $\Lambda$ is estimated, the covariance are
updated in the usual way:
\begin{equation}
  \label{eq:update-slda-covariance}
  \sigma^2 = \sum_d (y_d-\hat\mu_d)(y_d-\hat\mu_d)^T
\end{equation}

\section{Conclusion}


In (\ref{eq:slda-full-conditional-1}), $\hat{\theta}_d$ and
$\hat{\theta}_{d,-n}$ are usually very similar, especially for long
documents, because the length $N_d$ is the unification factor that
scales $\Omega_d$ and $\Omega_{d,-n}$ to $\hat{\theta}_d$ and
$\hat{\theta}_{d,-n}$ respectively.  This makes
$\N(y_d;\;\eta\cdot\hat{\theta}_d+\mu,\sigma^2)$ and
$\N(y_d;\;\eta\cdot\hat{\theta}_{d,-n}+\mu,\sigma^2)$ have very
similar values, and explains why sLDA can be approximated well by LDA
plus linear regression.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "llt"
%%% End:
